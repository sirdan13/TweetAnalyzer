package tweet.analyzer.core;

import java.io.BufferedReader;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.Serializable;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.Iterator;
import java.util.List;
import java.util.TreeSet;

import tweet.analyzer.utilities.IDFCalculator;
import tweet.analyzer.utilities.WordStats;


/**
 * Knowledge Base class. Contains word vectors and IDF values from the reference corpus
 * @author TvPad
 *
 */
public class KnowledgeBase implements Serializable {
	



	private static final long serialVersionUID = 5572716557438647819L;

	private TreeSet<String> sentences;
	
	private HashMap<String,WordStats> stats;
	
	private HashMap<String, ArrayList<Double>> vectors;
	
	private int vectorSize;
	
	public int getVectorSize() {
		return vectorSize;
	}

	private int totalDocs;
	
	
	
	public HashMap<String, WordStats> getStats() {
		return stats;
	}



	public int getTotalDocs() {
		return totalDocs;
	}



	/**
	 * Create corpus from a list of tweets
	 * @param tweetlist a list of tweets
	 * @throws IOException 
	 * @throws FileNotFoundException 
	 */
	public void createfromTweetList(List<Tweet> tweetlist) throws FileNotFoundException, IOException{
		
		Iterator<Tweet> iterator = tweetlist.iterator();
		while (iterator.hasNext()) {
			Tweet t = iterator.next();
			t.preprocessText();
			this.sentences.add(t.getProcessedtext());
		}
	}
	

	public KnowledgeBase() {
		this.sentences = new TreeSet<String>();
	}


	public TreeSet<String> getSentences() {
		return sentences;
	}

	public void setSentences(TreeSet<String> sentences) {
		this.sentences = sentences;
	}
	
	/**
	 * Compute the IDF of all the words in the reference corpus 
	 * @param corpusFile the reference corpus. Text file with one doc per line
	 * @param preprocess preprocess the input if set to true
	 * @throws IOException
	 */
	public void computeIDFFromCorpusFile(String corpusFile, boolean preprocess) throws IOException{
		
		
		HashMap<String, WordStats> statistics = new HashMap<String, WordStats>();
		
		//open corpus (text) file
		File inputfile = new File(corpusFile);
				
		BufferedReader in = new BufferedReader(
				   new InputStreamReader(
		                      new FileInputStream(inputfile), "UTF8")); 
		 
				String str;
				int countIterations = 0;
				
				//save documents to arraylist
				while ((str = in.readLine()) != null) {
					
					countIterations++;
					if(countIterations%1000==0)
						System.out.println("Processed docs: "+countIterations);
							
					//Preprocess
					if(preprocess){
						IDFCalculator idfc = new IDFCalculator();
						str = idfc.preprocessText(str);
						
					}
					
					//Tokenize
					String[] words = str.split("\\s+");
					
					HashMap<String,Integer> wordlist = new HashMap<String,Integer>();
					
					//compute word occurrences
					for(int i = 0; i < words.length; i++){
									
						if(wordlist.containsKey(words[i])){
						     wordlist.put(words[i], wordlist.get(words[i])+1);
						}
						else
						{
						    wordlist.put(words[i], 1);
						}				
					}
					
					//save results
					Iterator<String> it = wordlist.keySet().iterator();
					
					while (it.hasNext()){
						
						String key = it.next();
						
						if(statistics.containsKey(key)){
							WordStats word = statistics.get(key);
							int count = wordlist.get(key) + word.getOccurrences();
							int numDoc = 1 + word.getNumOfDocs();
							WordStats ws = new WordStats(count,numDoc);
							statistics.put(key, ws);
						}
						else
						{
							
							WordStats ws = new WordStats(1,1);
						    statistics.put(key, ws);
						}	
						
	
					}
					
												
				}
				
				in.close();
				
		this.totalDocs=countIterations;
		this.stats = statistics;;
				
	}
	

	/**
	 * Reads word vectorial representation from a text file (generated by the word2vec tool)
	 * @param vectorsFile text file containing vectorial representation of the corpus words
	 * @throws IOException
	 * @throws FileNotFoundException
	 */
	public void readVectorsFromWordToVecFile(String vectorsFile) throws IOException, FileNotFoundException{
		
		
		HashMap<String, ArrayList<Double>> vectors = new HashMap<String, ArrayList<Double>>();
		//scan file
		File inputfile = new File(vectorsFile);
		
		BufferedReader in = new BufferedReader(
				   new InputStreamReader(
		                      new FileInputStream(inputfile), "UTF8")); 
		 
				String str;
				int countIterations = 0;
				int totalWords = 0;;
				int dimensions = 0;
				
				//save documents to arraylist
				while ((str = in.readLine()) != null) {
					
					countIterations++;
					
					//the first line contains total words and vectors dimensions
					if(countIterations==1){
						String[] data = str.split("\\s+");
						totalWords = Integer.parseInt(data[0]);
						dimensions = Integer.parseInt(data[1]);	
						this.vectorSize = dimensions; 
					}
					
					//the other lines contain the word and the corresponding vector
					else{
						String[] data = str.split("\\s+");
						String word = data[0];
						
						//System.out.println(word+" "+data[100]+" "+data.length);
						
						
						ArrayList<Double> vector = new ArrayList<Double>(data.length-1);
						//System.out.println("SIZE "+vector.size());
						
						for (int i=0; i<data.length-1;i++){
							//System.out.println("DATA[1] "+data[1]+" DATA[100] "+data[100]);
							vector.add(i, Double.parseDouble(data[i+1]));
							
						}
						//System.out.println("Vector size:  "+vector.size());
						
						vectors.put(word, vector);
																				
					}
											
					
					if(countIterations%1000==0)
						System.out.println("Processed words: "+countIterations+"/"+totalWords);
							
				}	
				
				System.out.println("Total Words "+totalWords);
				System.out.println("dimensions "+dimensions);
				
				in.close();
				
				this.vectors = vectors;
		
		
	}
	
	
	/**
	 * Create KB object from text files (corpus file and vectors file) 
	 * @param corpusFile the file containing the reference corpus (text file with one line per document)
	 * @param vectorsFile the file containing the vectorial representation of the words in the corpus (created with word2vec)
	 * @throws IOException
	 */
	public void createFromFiles(String corpusFile, String vectorsFile) throws IOException{
		
		this.computeIDFFromCorpusFile(corpusFile, false);
		this.readVectorsFromWordToVecFile(vectorsFile);
		
		
	}



	public HashMap<String, ArrayList<Double>> getVectors() {
		return vectors;
	}
	
	
}
